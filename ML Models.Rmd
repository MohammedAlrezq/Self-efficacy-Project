---
title: "ML Models"
author: "Mohammed Alrezq"
date: "2024-10-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#Package/libraries 
#install.packages("tidyverse")
library(tidyverse)
#install.packages("cli")
library(cli)
#install.packages("rlang")
library(rlang)
library(psych)# for factor analysis 
library(psychTools)

library(ggplot2)
library (descstat)
library(dplyr)

library(REdaS)# to perform Bartlettâ€™s Test of Sphericity before FA 
library(GPArotation) # required by FA 
library(GGally)
library(lavaan)# used for CFA and SEM modeling 
library(lavaanPlot)
library(semPlot)# also use to build graph for CFA, and SEM
library(caret)
```

```{r}
plotTheme <- function() {
  theme(
    plot.title = element_text(size = 14, family = "sans", face = "plain", hjust = 0),
    plot.subtitle=element_text(size = 11, family = "sans", hjust = 0),
    plot.caption=element_text(size = 10, family = "sans", face = "italic", hjust = 0), 
    axis.title.x = element_text(size = 10, family = "sans", face = "plain", hjust = 1, vjust = -0.5),
    axis.title.y = element_text(size = 10, family = "sans", face = "plain", hjust = 1, vjust = 1),
    axis.text = element_text(size = 9, family = "sans", face = "plain"),
    panel.background = element_blank(),
    panel.grid.minor = element_line(colour = "gray"),
    panel.grid.major = element_line(colour = "gray"),
    axis.ticks = element_blank(),
    legend.title = element_text(size = 10, family = "sans"),
    legend.text = element_text(size = 9, family = "sans"),
    axis.line = element_blank()
  )
}
```

## R Markdown
```{r cars}
school_model <- read.csv("school_model2.csv")

school_model <- school_model |> 
  select(-1)

names(school_model)
```



# Machine Learning 
# Split data to training and testing and select variables significant based on regression 
```{r}
set.seed(1234)
ind <- sample(2, nrow(school_model), replace = T, prob = c(0.8, 0.2))
train <- school_model[ind == 1,]
test <- school_model[ind == 2,]

#names(school_model)
#write.csv(school_model, "school_model_afterVARNameChange.csv")
#write.csv(school_model, "school_model.csv")
```


# cv SET UP 
```{r}
# Regression Tree
# Bagging
set.seed(1234)
cvcontrol <- trainControl(method="cv", #here used CV (used with regression or classification)
                          number = 10,
                          allowParallel=TRUE)

```

```{r}
##Part 2: Random forest 
# RF: same as DT except change method =RF #-Gender -Relg.Spit -EMP
set.seed(1234)
forest <- train(SEFF ~ .,  
                data=train,
                method="rf", # change the method to RF
                trControl=cvcontrol,
                tuneLength= 12,
                importance=TRUE)
# Plot mtry and find the best value
plot(forest)
forest$bestTune

# re-run the model using (mtry=2)
set.seed(1234)
forest_mtry <- train(SEFF ~ .,  
                data=train,
                method="rf", # change the method to RF
                trControl=cvcontrol,
                tuneLength= 3,
                importance=TRUE)

# plot important variabels 
plot(varImp(forest_mtry))




# Plot, RMSE, R-square based on test data 
set.seed(1234)
rf <-  predict(forest_mtry,  test)
plot(rf ~ test$SEFF, main = 'Predicted Vs Actual MEDV - Test data')
sqrt(mean((test$SEFF - rf)^2))
cor(test$SEFF, rf) ^2

head(rf)
head(school_modelReg$Self.Efc)
# Explain predictions: ML is black box and this give more description about inside
explainer <- lime(test[1:3,], forest, n_bins = 5)
explanation <- explain( x = test[1:3,], 
                        explainer = explainer, 
                        n_features = 5)
plot_features(explanation)
plot_explanations(explanation)


install.packages("rattle")
library(rattle)
fancyRpartPlot(forest$finalModel)

plot(forest$finalModel)

###Random forest based on class note############

library(randomForest)
set.seed(1)
bag.boston=randomForest(Self.Efc~.,data=train,mtry=9,trControl=cvcontrol,
importance=TRUE)
```





# Try another way to run it faster

# Machine Learning with CoreParallael


```{r}
library(randomForest)
library(iml)
##Part 2: Random forest 
# RF: same as DT except change method =RF #-Gender -Relg.Spit -EMP

library(doParallel)
cl <- makeCluster(detectCores() - 1) # Use all available cores minus one
registerDoParallel(cl)


set.seed(1234)
forest <- train(SEFF ~ .,  
                data=train,
                method="rf", # change the method to RF
                trControl=cvcontrol,
                tuneLength= 11,
                importance=TRUE)

stopCluster(cl) # Stop the parallel cluster after training


```




#Creat SHAP plot
```{r}

# Step 4: Create Predictor Object for SHAP Analysis Using Test Data
# Create a model that can be used by the iml package
rf_predictor <- Predictor$new(
  model = forest$finalModel, # Use the trained Random Forest model
  data = test,              # Use the test data for SHAP analysis
  y = test$SEFF             # Specify the target variable from the test data
)



# Round the feature values to two decimal places for clearer labels
test_rounded <- test
test_rounded[] <- lapply(test_rounded, function(x) if(is.numeric(x)) round(x, 2) else x)


# Step 5: Generate SHAP Values for Multiple Observations
# Loop over a subset of test data for a comprehensive analysis
set.seed(1234)
shapley_all <- Shapley$new(rf_predictor, x.interest = test_rounded[1:1000, ]) # SHAP for the first 5 observations
shapley_all$plot()+ 
  plotTheme() # Apply custom theme to the SHAP plot


# Step 6: Feature Importance Using SHAP on the Test Data
# Feature importance plot based on SHAP values across the test data
shap_importance <- FeatureImp$new(rf_predictor, loss = "mse")
shap_importance$plot()+
  scale_y_discrete(labels = scales::number_format(accuracy = 0.01)) + # Set y-axis to two decimal places
  plotTheme() # Apply custom theme to the feature importance plot


```



#another try
```{r}

library(doParallel)
cl <- makeCluster(detectCores() - 1) # Use all available cores minus one
registerDoParallel(cl)


# re-run the model using (mtry=2)
set.seed(1234)
forest_mtry <- train(SEFF ~ .,  
                data=train,
                method="rf", # change the method to RF
                trControl=cvcontrol,
                tuneLength= 3,
                importance=TRUE)

stopCluster(cl) # Stop the parallel cluster after training


# plot important variabels 
plot(varImp(forest_mtry))

# Plot, RMSE, R-square based on test data 
set.seed(1234)
rf <-  predict(forest_mtry,  test)
plot(rf ~ test$SEFF, main = 'Predicted Vs Actual MEDV - Test data')
sqrt(mean((test$SEFF - rf)^2))
cor(test$SEFF, rf) ^2
```


#crea sahap BASED ON TRAINING SET 
```{r}
# Step 4: Create Predictor Object for SHAP Analysis
# Create a model that can be used by the iml package
rf_predictor <- Predictor$new(
  model = forest$finalModel, # Use the trained Random Forest model
  data = train,             # Use training data
  y = train$SEFF            # Specify target variable
)

# Step 5: Generate SHAP Values
shapley <- Shapley$new(rf_predictor, x.interest = test[1:10, ]) # SHAP for a single observation
shapley$plot()

# Generate SHAP plot for all data
# Feature importance plot based on SHAP values for all test data
shap_values <- FeatureImp$new(rf_predictor, loss = "mse")
shap_values$plot()
```

